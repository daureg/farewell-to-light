\chapter{Measuring similarity}
\label{ch:background}

In this chapter, we provide an overview of the theoretical background of many
methods we employed to solve our problem. Because it does not contribute
directly to our solution, it can be skipped by readers already familiar with
clustering and metric learning. Yet it introduces many concepts on which we
rely later. As \enquote{finding similar neighborhoods} entails, the main theme
is measuring similarity between objects. This can be used to cluster them in
meaningful groups (\autoref{sec:clustering}). An important step is to tailor
general metric toward our goal (\autoref{sec:metric}). Indeed,
\textcite{GoodSimilarity08} show that \enquote{good} properties of similarity
functions are transferred to classifiers using them.

\section{Clustering}
\label{sec:clustering}

A general class of methods addressing unsupervised problems is clustering.
\enquote{\emph{Clustering algorithms partition a given data set into several
groups based on some notion of similarity between objects}}
\autocite{LimitsClustering05}. The most widely used is the
\methodname{$k$-means algorithm} \autocite{kmeans67}. After initially choosing
$k$ centroids, an iterative process takes place, divided in two phases. First
each point is assigned to the cluster of the closest centroid. Then centroids
positions $\mu_i$ are updated by taking the mean of all points belonging to their
cluster. Convergence occurs when centroids positions do not change between
iterations. In practice, the algorithm is fast but it is only guaranteed to
find a local optimal of the within-cluster sum of squares: \[ \sum_{i=1}^{k}
\sum_{ x_j \in \mathrm{cluster}_i} \left\| x_j - \mu_i \right\|^2 \] Another drawbacks is that
$k$, the number of clusters, has to be specified before algorithm execution,
whereas this information is often unknown at this stage. Finally, because it
results in a Voronoy diagram, the clusters found are linearly separable, which
may not reflect the actual data.

Because of these limitations, numerous alternatives have been developed. A
cluster can be defined as an area of high density surrounded by low density
regions. This definition suggests assigning a numeric value of density to every
point of space. A common method is \methodname{Kernel Density
Estimation} \autocite{KDE56}. A kernel, generally parametrized by its
bandwidth $h$, is centered around each point
(\ie{} a symmetrical weighting function, for instance a Multivariate Gaussian)
and the estimation of the probability distribution $f$ is their normalized sum:
% http://en.wikipedia.org/wiki/Multivariate_kernel_density_estimation
\[ \hat{f}_\bold{h}(\bold{x})= \frac{1}{n} \sum_{i=1}^n K_\bold{h} (\bold{x} -
\bold{x}_i) \]

Although it is a useful information by itself, this density estimation does
not directly provide a clustering. One idea would be to find the modes of
$\hat{f}$, which is the underlying principle of Mean Shift
\autocite{MeanShift95}. Another density based algorithm is
\methodname{\textsc{DBSCAN}} \autocite{DBSCAN96}. It has two parameters: a
distance $\epsilon$ and a number of points $\mathrm{minPts}$. A core sample is
a point with at least $\mathrm{minPts}$ neighbors in its
$\epsilon$-neighborhood. From a core sample, its neighbors are visited to find
other core samples belonging to the same cluster. Once it is not more possible,
the algorithm moves to another point. Points that are $\epsilon$ close to a
core sample without having a large enough neighborhood are also part of the
cluster (but are deemed fringe points rather than core ones). Any other
remaining point is noise. With an appropriate index structure (see
\autoref{sub:spatial-structures}), neighborhood queries takes $O(\log n)$ time
and the algorithm runs in $O(n\log n)$. Otherwise, one need to compute the
pairwise distance matrix, which takes $O(n^2)$ time and memory. DBSCAN finds
an arbitrary number of clusters with arbitrary shapes. In the special case where
points carry user identification (like photos or tweets), it can be tweaked to
favor areas that exhibit large user diversity \autocite{PDBSCANKisilevich2010}.
Another extension, OPTICS, can identify clusters despite data having varying
density, but it only produces a reachability-plot, which requires further
processing to be transformed into clusters \autocite{OPTICS99}.

\iffalse
\begin{comments}
	I didn't use these methods so I probably don't need to talk about it\\
	Some are based on graphs. For instance \methodname{Spectral
	Clustering} \autocite{SpectralClustering01} (which is related to
	Kernel $k$-means, as shown by \textcite{KernelKmeans04}).
	\methodname{Affinity Propagation} \autocite{AffinityPropagation07}
% https://en.wikipedia.org/wiki/Affinity_propagation
% http://scikit-learn.org/stable/modules/clustering.html#affinity-propagation

	Others are hierarchical
\url{https://en.wikipedia.org/wiki/Hierarchical_clustering}
\end{comments}
\fi

A major component of all clustering algorithm is the distance function, and
the space where it operates. The default choice is the Euclidean metric in the
original feature space but these two parameters can be changed. In the latter
case, it often leads to dimensionality reduction, whereas the former suggests
metric learning, which will be discused in \autoref{sec:metric}.

\subsection{Dimensionality reduction}

Many methods are based on the idea of
preserving distances between points, under the general name Multidimensional
Scaling \autocite{MDS77}. Given points $i$ and $j$ in the original space, we
know their separation $\delta_{i,j}$ with a weight $w_{i,j}$\footnote{This
weight denotes either confidence in the measurement or importance of the pair
of points.}. We are
looking for their new position $x_i$ and $x_j$ in the reduced space such that
the stress \[ S = \sum_{i,j} \sqrt{\frac{\sum w_{i,j}(\delta_{i,j} - d(y_i,
y_j))^2}{\sum d(y_i, y_j)^2}} \] is minimized.

Instead of computing distances in a geometrical sense, it is also possible
to give them a probabilistic interpretation. Namely, in Stochastic Neighbor Embedding
\autocite{SNE02}, starting from the dissimilarity between two points $i$ and
$j$, $d^2_{ij} = \frac{\vnorm{x_i - x_j}^2}{2\sigma^2_i}$, we compute the
probability of $i$ picking $j$ as a neighbor in the original space \[ p_{ij} =
\frac{\exp(-d_{ij}^2)}{\sum_{k \neq i}\exp(-d_{ik}^2)}\]

We do the same in the low dimensional space, where positions are denoted by
$y_i$ \[ q_{ij} = \frac{\exp(-\vnorm{y_i - y_j}^2)}{\sum_{k \neq
i}\exp(-\vnorm{y_i - y_k}^2)}\] and we set the $y_i$ to minimize $\sum_i
\KL{P_i}{Q_i}$\footnote{Here, $\mathrm{KL}$ is the Kullback--Leibler
divergence, which will presented shortly.}, which can be understood as preserving the neighborhood of each
point. Later was introduced \tsne{} \autocite{tSNE08}, which makes the
optimization problem simpler and avoid the crowding problem by modeling
distances in the low dimensional space with a heavy tail Student $t$-distribution
with one degree of freedom \[ q_{ij} = \frac{\left(1+\vnorm{y_i -
y_j}^2\right)^{-1}}{\sum_{k \neq i}\left(1+\vnorm{y_i - y_k}^2\right)^{-1}}\]
An ingenious implementation runs in $O(n\log n)$ \autocite{BarnesHut13}.

It is easier to visually evaluate clustering results in this two or three
dimensional space, although dimensions are not necessarily meaningful.
Furthermore, we assume that sample points are not randomly distributed in the
whole space. Thus, we are trying to recover the low dimensional manifold where
they lie.

\subsection{Spatial data structure}
\label{sub:spatial-structures}

A common subproblem of many methods exposed so far\footnote{This subproblem is also
interesting by itself.} is to retrieve the $k$ closest neighbors of a given
point. If there are $n$ candidates in a $d$ dimensional space, a naive linear
scan takes a prohibitive $O(n\cdot d)$ time.  Fortunately, there are data
structure well suited to solve this problem faster, even tough they become
less efficient as $d$ increases. Namely, we will describe $k$-d tree and ball
tree. The common idea behind them is to partition space according to points
distribution.

A $k$-d tree \autocite{kdtree75} is a binary search tree whose every node is a
hyperplane that separates the space $\mathbb{R}^k$ in two parts, and whose
leaves contain points lying in the region of the space define by all the
parents hyperplanes. These hyperplanes are aligned with axes, meaning that on
the left of a node at the $i$\textsuperscript{th} level are all points whose
coordinate $i$ is smaller than $t$ and the right the points for which is
larger. This threshold $t$ is chosen as the median value of this coordinate (or
at least a good approximation of it) to keep the tree balanced. The tree is
constructed recursively until there are not enough points at a given sub level
to justify further splitting. After that, the main operation is retrieving the
point who is the closest to a given input point. By looking at its coordinates,
we can go down the tree in $O(\log n)$ time to restrict the number of points to
look for.

Ball trees \autocite{BallTree89} are also binary trees but this time, nodes are
hyperspheres of $\mathbb{R}^k$, or balls. The ball at each node is the smallest
ball that contains all the balls in the corresponding subtree, and leaves are
balls which enclosed some points of the dataset. Therefore, for a ball $S$ (of
center $c$ and radius $r$) and a query point $q$, we know that if $q \notin S$,
$q$ cannot be to closer to points in $S$ than $\vnorm{c - q} -r$. This fact is
used at query time to prune parts of the tree.

Another data structure of interest is the R-tree \autocite{Rindex84}, which is
able to store and retrieve non-zero size geometry such as polygon. There, nodes
are hyperrectangles and leaves contain only one object. It is again a balanced
search tree, but not a binary one. Each node can store up to $M$ entries. The idea
behind search is the same, taking advantage of the nested box to prune part of
the tree. The main difficulties lie in the efficient construction of the tree
as well as fast subsequent insertions.

\section{Metric}
\label{sec:metric}

\subsection{Ground metric}
\label{sub:metric-learning}

As we mentioned, to improve performance of classifiers who rely on
distances\footnote{The textbook example being $k$-nearest neighbors
classifier.}, we must change the way distances are measured. Either by
computing  them in a new space, as in dimensionality reduction, or by
directly modifying the underlying norm, that is replacing the
$L_2$ norm $\vnorm{x-y}_2^2$; for instance by the $L_1$ norm (Manhattan distance)
or the $L_\infty$. Noting that $\vnorm{x-y}_2^2 = (x-y)^T A (x-y) = d_A(x,y)$,
where $A=I$, one can also replace $A$ with any positive definite matrix to still
define a metric. A well-motivated choice is setting $A$ as the inverse of the
covariance matrix, corresponding to the Mahalanobis distance
\autocite{Mahalanobis36}. But $A$ can be learned from the data in other
semi-supervised ways, by specifying constraints between pairs of points. These
constraints are based on class labels, meaning we want points of the same class
to be close and points of different classes to be set apart. In this work, we
consider two methods that optimize $A$ subject to such pairwise constraints.

The first is \methodname{Information Theoretic Metric Learning}
\autocite{InfoMetric07}. Given a positive definite matrix $A$, we can
associate it with a multivariate Gaussian distribution of mean $\mu$ and
covariance $A^{-1}$ (uniquely, up to a scaling constant): \[p(x; A) =
\frac{1}{Z}\exp\left(-\frac{1}{2}d_A(x, \mu)\right)\] We also need a reference
matrix $A_0$ (for instance the identity or the covariance matrix of the
dataset), a set $S$ of similar points and a set $D$ of dissimilar points. We
want $A$ to be close to $A_0$ in the sense of relative entropy $\KL{p(x;
A_0)}{p(x,A)} = \int \! p(x; A_0) \log\frac{p(x; A_0)}{p(x; A)}\, \mathrm{d}x$
by solving

\begin{align*}
	\min_A &\quad \KL{p(x; A_0)}{p(x; A)} &\quad\\
	\text{subject to} &\quad d_A(x_i, x_j) \leq u &(i,j) \in S,\\
 &\quad d_A(x_i, x_j) \geq l &(i,j) \in D.
\end{align*}

The other one is \methodname{Large Margin Nearest Neighbor} \autocite{LMNN09}
\begin{figure}[ht]
	\includegraphics[width=0.9\linewidth]{schema_lmnn}
	\caption{Neighborhood of a point, from \autocite{LMNN09}.\label{fig:lmnn}}
\end{figure}
Specifically, let $N_i$ be the set of the $k$ closest neighbors of $x_i$ in the
original space which share the same class, and called them target neighbors. Closer
points of different class are impostors (see \autoref{fig:lmnn}
% \vpageref{fig:lmnn}
). The optimization pushes target neighbors close to $x_i$
while pulling away impostors. The number of active constraints is linear
rather than quadratic because only the neighborhood of each points contributes
to it. Thus the following semidefinite program can be solved efficiently:
\begin{equation*}
	\min_A \sum_{i, j\in N_i}
	\underbrace{d_A(x_i, x_j)}_\text{\emph{pull} target neighbor $x_j$ closer} +
	\underbrace{\mu \sum_{k, y_k\neq y_i} \left[ 1 + d_A(x_i, x_j) -
	d_A(x_i, x_k)\right]_+}_\text{\emph{push} impostor $x_k$ beyond target
	neighbor $x_j$}
\end{equation*}
where $[x]_+$ is the hinge-loss: $[x]_+=\max(0, x)$

The end result is a linear, global metric but there are other approaches. One
can learn multiple local metrics, or a non linear metric like
\methodname{Gradient Boosting LMNN} \autocite{GBLMNN12}. It solves a similar
optimization problem except that distances are computed in a new space,
obtained by a non linear additive mapping. This mapping is itself learned by
combining multiple regression trees of limited depth selected by gradient
boosting.

A more complete overview of metric learning is given in the survey of
\textcite{MetricSurvey13}.

\subsection{Set metric}

We discussed metrics between points but in this work, we also used distances
between sets of such points. If we interpreting their coordinates as geometric positions in
$\mathbb{R}^k$, there are some easy to compute metrics. It includes, for instance, the
bottleneck distance \autocite{Bottleneck96} (the smallest distance between
pairs of farthest points) and the Hausdorff distance (the largest distance
between pairs of closest points) along with its variations
\autocite{ModifiedHausdorff94}.

But such approaches are rather sensible to outliers and do not perform so well
in high dimension, as distances tend to be more uniformly distributed there.
Therefore, we want to involve all the points from both sets in the computation.
One way to do this is to build a bipartite graph with edges weighted by the
opposite distance between points and find a maximum weight matching.
This can be solved by the Hungarian algorithm
\autocite{Hungarian57} in $O(n^4)$, a precursor of primal-dual method to solve
linear program. This bound was later improved to $O(n^3)$ by considering it as
a flow problem \autocite{HungarianN372}.

Because this problem has a min cost flow constraints matrix, it is are totally
unimodular (\ie{} every non singular sub square matrix has a determinant of
$+1$ or $-1$) and thus admits an integer solution. Yet it may be more convenient
to consider non integer supply and demand, which leads to fractional weight and
probabilistic interpretation. This is then called the transportation problem
and the modern Kantorovitch formulation makes the probabilistic aspect more
apparent:
\begin{equation*}
 \underset{\gamma \in \Gamma(\mu, \nu)}{\mathrm{inf}}
 \int_{X\times Y} c(x,y)\mathrm{d}\gamma(x,y)
\end{equation*}
Here $X$ and $Y$ are two metric spaces, $c$ a measurable function between them
which represents the cost of moving from different parts of $X$ to some parts of $Y$.
We look for a measure $\gamma$ whose marginals on $X$ and $Y$ are $\mu$ and
$\nu$ (the set $\Gamma(\mu, \nu)$ is called the set of all \emph{couplings} of
$\mu$ and $\nu$).

It is closely related to the Wasserstein distance of
two probability measures $\mu$ and $\nu$ of a space $M$ with
$p$\textsuperscript{th} finite moments
\begin{equation*}
W_p(\mu, \nu) = \left( \underset{\gamma \in \Gamma(\mu, \nu)}{\mathrm{inf}}
\int_{M\times M} c(x,y)^p\mathrm{d}\gamma(x,y)\right)^{\frac{1}{p}}
\end{equation*}
$W_1$ is also called \methodname{Earth Mover's Distance} \autocite{EMD98}.
Intuitively, it measures the {\em total amount of work} needed to transform
(move) one set of positions (total mass) to the other. It can also be defined
as the minimal cost of the following linear program
\begin{align*}
	&\quad \underset{f}{\min} \sum_{i,j} d_{i,j} f_{i,j} \\
	\text{subject to} &\quad \sum_j f_{i,j} \leq w_{P,i} \\
	&\quad \sum_i f_{i,j} \leq w_{Q,j} \\
	&\quad \sum_{i,j} f_{i,j} = \sum_i w_{P,i} = \sum_j w_{Q,j} = 1
\end{align*}
where $d_{i,j}$ is the distance between the $i$\textsuperscript{th} point of
$P$ and the $j$\textsuperscript{th} point of $Q$, and $w_P$ and $w_Q$ are the
weights of these points, summing up to $1$.

As we used \emd{} extensively, it worth mentioning that since it was
introduced, there have been efforts to improve its performance while preserving
its characteristics. For instance, \textcite{Ling2007} present a lower
complexity algorithm that uses $L_1$ as the ground metric. \Textcite{Shirdhonkar2008}
propose another approximation algorithm based on wavelet decomposition but the
complexity is exponential with the number of dimension, which makes it
intractable when $d$ is greater than 6. \Textcite{Pele2009} show that assuming
an upper bound over all pairwise distances, one can further simplify the
optimization formulation, which improves running time and better match the
human perception of colors in computer vision. Finally, the Sinkhorn distance
\autocite{FastEMD13} is closely related to optimal transport and can be used to
quickly obtain lower and upper bounds of \emd{}, especially when run over GPUs.
But it also performs well on classification tasks on its own.

As hinted in \emd{} by the link between distance in Euclidean spaces and
probability spaces, we can also measure similarity between sets of objects if we
describe them by probability distributions. Information theory provides us with
multiple tools for assessing distance between distributions, the most well known
being the Kullback--Leibler divergence, defined in the discrete case as
\begin{equation*}
	\KL{p}{q} = \sum_i p_i\log\left(\frac{p_i}{q_i}\right) 
\end{equation*}
However, this divergence is not a metric as it is not symmetric (nor does it
satisfies the triangular inequality). Thus, we used a metric based on it, the
\methodname{Jensen--Shannon Divergence} \autocite{JensenShannon03}, which is
defined using the average of the two distributions $m=\frac{p+q}{2}$
\begin{equation*}
	\JSD{p}{q} = \frac{1}{2}\left(\KL{p}{m} + \KL{q}{m}\right) 
\end{equation*}
