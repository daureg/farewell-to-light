\chapter{Methods}
\label{chap:methods}

\section{Closest Neighbors}

\subsection{Dimensionality reduction}

\begin{flushright}{\slshape
 If people could see in high dimensions \\
 machine learning would not be necessary.
} \\ \medskip
--- Pedro \Textcite{MLKnowledge12}
\end{flushright}

A common idea in Machine Learning problems is that data are meaningful.
Although we fail to distinguish their structure in high dimensional space, we
assume that data are the result of a principled underlying process and
therefore that sample points are not randomly distributed in the whole space.
A fruitful approach to validate this hypothesis is to perform dimensionality
reduction, with the aim of recovering a low dimensional manifold that embed
the data.

So far, t-Distributed Stochastic Neighbor Embedding \autocite{tSNE08} seems
the most promising method.

\subsection{Metric learning}

After reading some surveys \autocite{MetricSurvey06, MetricSurvey13}, I tried
Information-theoretic Metric Learning \autocite{InfoMetric07} and
Gradient-Boosted Largest Margin Nearest Neighbors \autocite{GBLMNN12} although
it would be interesting to also use histogram distance
\autocite{HistogramDistance02}

\section{Matching under constraint}

\section{Visualization}
