\begin{flushright}{\slshape    
		The future is already here \\
		it's just not very evenly distributed.
    } \\ \medskip
    --- William Gibson
\end{flushright}

\chapter{Discussion}
\label{chap:discussion}

In this chapter, we offer some concluding remarks regarding the quality of our
results (\autoref{sec:cquality}), the overall work performed
(\autoref{sec:cconc}), and some future directions (\autoref{sec:cfuture}).

\section{Quality of results}
\label{sec:cquality}

It's a difficult problem because the query is not clearly defined and
potentially convey high level semantics.

\section{Conclusion}
\label{sec:cconc}

In this thesis, we define the problem of matching neighborhoods across cities
and introduce a method to solve it efficiently. We start by crawling social
networks for traces of geo-located human activity in 20 major European and
American cities. We collect over 8 million geo-tagged Flickr photos from the
last six years and more than 4 million Foursquare check-ins. We conduct an
extensive exploration of this dataset to reveal pattern of human activity and
better understand the relation between our two different sources of
information.

Selecting venues as the elementary constituents of a city, we characterize them
by features such as time of activity, diversity of audience, popularity and
repartition of surrounding venues. We then evaluate several measures of
similarity between venues, several of them based on semi supervised metric
learning approach and conclude that, using \fs\ categories as a proxy, \eucl{}
and \lmnn{} are the most suitable. Next, we consider various ways of measuring
distance between sets of venues and based on ground truth collected through a
user study, we find out that \emd{} outperforms other candidates.

The exhaustive search used in this selection process is time consuming as it
evaluates distance function for many regions.  Therefore, we develop a method
to prune the search space, by pre-processing venues to keep only ones close to
query venues and clustering them into promising neighborhoods. It greatly
enhances run time while incurring only minor loss of accuracy compared with
brute force search.

\section{Future work}
\label{sec:cfuture}

The methods developed in this thesis---as well as the results they produce---
are already satisfactory. Yet we suggest various ways in which they can be
improved. We also show how the original problem can be extended to provide
even more insights into cities organisation and usage by inhabitants.

\medskip

The most straightforward way of improving result quality is to use more data
sources to derive more features. It is indeed common knowledge that more data
trumps better algorithms \autocite{MoreData09}. In the introduction, we mention
for instance data about transportation, weather, air quality, energy
consumption, communication, demographics, etc. Although this will inevitably
increase the noise, we expect that it will enable more accurate description of
venues and areas, making similarity between them even more meaningful.

One comparing metrics between venues, we used metric learning to tune some of
them based on the data and conclude that \eucl{} and \lmnn{} where the most
accurate. Therefore, we used them as ground metric for \emd{} when measuring
distance between sets of venues. Yet it also possible to learn the ground
metric at this stage, by taking advantage of labeled data regarding sets of
venues (and not only venues), as described by \textcite{LearnEMD14}.

Finally, a nice modification will make the algorithm deliver interpretable
results. Beyond a single number, the similarity, or absence thereof, must be
motivated. It enables principled comparisons of cities and more user friendly
outputs (as well as justifying why no sensible result were found for some
queries).  Furthermore, this would address recent concerns about Algorithmic
Accountability \autocite{Accountability13}. For instance,
\textcite{Discrimination13} showed that in case of online advertisement
discrimination, opacity of the algorithms makes it difficult to find and
sanction those responsible.

In the current situation, the more immediate way to do it would be to find
justification after the matching process (by clustering venues and summarizing
the results with some text like \enquote{These two regions match because people
take a lot of photos, go there on weekdays from 4pm to 8pm and there are a
lot of cultural buildings.} Another source of information is the resulting
flow of \emd{}. By telling how much each venue in one region is close to venues
in the matching neighborhood, we can recover some explanations.  Yet it would
be more satisfactory if the whole process was driven by such considerations
instead of being a mere post processing step.

\medskip

As mentioned in some related works \autocites{Livehoods12}{Hoodsquare13},
having a measure of similarity between venues allows clustering them into
dense groups sharing similar characteristics. If we include spatial
constraints, it corresponds exactly to the definition of neighborhoods, but
generated by data instead of some historic administrative decisions. This
also reduces the friction for the user of the system. Instead of having to
choose a whole region, simply pointing a single location will generate an
appropriate neighborhood for the matching process.

A related problem is to match several (or all) neighborhoods from one city at
the same time but without excessive overlapping. That is given a cover of one
city, try to find the best coverage of another city along with a mapping
between them.  This is more computationally challenging but would allow
answering questions such as: how homogeneous are European cities compared with
American ones? What is the most European city in the United States? Are there
common sub structures shared by all cities worldwide? Conversely, what are the
places that are unique to Paris or Helsinki and do not exist anywhere else?

Finally, it would be fruitful to apply the same method to similar problems
(such as those mentioned in the Introduction \vpageref{:more-problems}) to see
how well it generalizes and maybe get new ideas on how to improve it.
