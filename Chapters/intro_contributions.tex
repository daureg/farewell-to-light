\section{Outline}
\label{sec:overview}

Here we give an outline of the thesis structure.  After exposing our problem in
details in \autoref{ch:problem}, the rest of this work will be devoted to
present which solutions we used to solve it and how well they perform. We start in
\autoref{ch:background} by introducing some common techniques concerning
similarity measure that serve as building blocks for our own methods.  Namely,
we talk about clustering, how it relates with dimensionality reduction and say
a few words about data structures for efficient implementation. Then we discuss
learning metrics between single object and introduce the metrics we used to
compare sets of such object. \autoref{chap:datasets} is dedicated to the
datasets on which we conducted our experiments. We describe how we collected
data from 20 cities in Europe and in the US; namely activity logs from \fs\ and
\flickr, a \gls{lbsn} and a photo-sharing platform respectively. We explore the
organization of these activities in time, in space, and with respect to various measures of
entropy, as well as the interplay between the two sources. This let us
choose a set of numeric features which accurately describes the characteristics
and the overall activity of locations inside cities.

These careful preparations finished, we start presenting results about
metrics in \autoref{chap:metric}. First we devised two information retrieval
tasks based on \fs{} categories to assess the ability of several metrics to match
venues together. We show a slight advantage of metrics whose parameters
are learned from the data compared with simple Euclidean distance. We remark
as well that accuracy is generally low because information as venue level is
rather noisy, which further motivate the study of neighborhood similarity.
Using ground truth data obtained from a user study conducted over Internet, we
compare several metrics and show that \emph{Earth Mover's Distance} is the
most able to agree with human perception.

To select \emd{}, we perform an exhaustive search, which is rather time
consuming. Therefore, in \autoref{chap:approx}, we describe the design of a
faster search strategy. It relies on a pre-processing of venues in the target
city with respect to the query in order to prune the search space. The
remaining venues are then clustered in space to form a few candidate
neighborhoods. We illustrate experimentally that the speedups thus obtained
come at little accuracy cost and in some cases, even provide better results.
Finally, we give a list of references to related works in
\autoref{chap:related} and we conclude in \autoref{chap:discussion} by
discussing other setting where our method can be applied and offering some
directions for future work.

\iffalse
Our approach is briefly described as follows.  By collecting rich geo-enabled
data from social-media platforms, and by associating this data to the different
venues of the city, we can represent each venue by a feature vector, which
accurately describes the characteristics and the overall activity of the venue.
We then ask to devise similarity measures between venues, as well as between
neighborhoods, i.e., sets of venues that are geographically close to each
other. 

We address these two problems from a {\em metric-learning} point of view
\cite{MetricSurvey13}.  We experiment with many different distance measures
and with algorithms that aim to learn their parameters. %%  of the underlying
metric.  To learn the parameters of the distance measures and select the
optimal settings we use ground-truth, either obtained automatically with simple
rules or gathered from carefully-designed user studies. 

Our results indicate that the problem of finding a venue similar to a single
query venue is a challenging problem, as the accuracy of the various tasks we
design is relatively low.  This is probably due to the fact that data at the
venue level are noisy.  However, when aggregating many venues together and
asking to find similar neighborhoods, the results become of much higher
quality.  This is indicated in our case study in
Section~\ref{section:casestudy}.

The  measure that is shown to perform best for the task of finding similar
neighborhoods is the {\em earth-mover's distance} (\emd) \cite{EMD98}.  \emd\
is known to be a robust measure---however, it is also expensive to compute.
Motivated by this observation, we address the issue of computational
efficiency.  In particular,  given a neighborhood \region\ in one city, we ask
how to find the most similar neighborhood to \region\ in another city (or a set
of other cities) under \emd, and without performing brute force computation.
We design a pruning strategy that yields significant speed improvement with
minimal loss in accuracy. 

Our study and our algorithms are based on extensive experimental evaluation, on
data collected from many cities in Europe and in the US.  Our datasets consist
of activity logs gathered from \fs\ and \flickr, a location-based social
network and a photo-sharing platform, respectively.  Yet our study can be
enriched by many other types of data, such as transportation, weather, air
quality, energy consumption, etc.  Such an extension is left for future work.

Finally we offer some concluding remarks about the quality of our results,
about the overall work performed, and about future directions.

\section{Contributions}

\begin{itemize}
	\item Collect and compute relevant features describing venues.
	\item Seamlessly incorporate heterogeneous \emph{items} by defining
		appropriate metrics and learning their parameters.
	\item Efficient search in $\mathbb{R}^2$ instead of brute force for
		the query~\ref{q:space}.
	\item Compare results with existing methods and evaluate them against
		human judging.
\end{itemize}
\fi
